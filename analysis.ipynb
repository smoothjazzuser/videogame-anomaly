{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    from tqdm.notebook import tqdm\n",
    "    import imageio.v2 as imageio\n",
    "    from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "    import matplotlib.cm as cm\n",
    "    import matplotlib.animation as animation\n",
    "    import gc\n",
    "    from glob import glob\n",
    "    import warnings \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    import os\n",
    "    from random import sample, seed, randint, random, shuffle, choice, choices, uniform, gauss, triangular\n",
    "    from PIL import Image, ImageOps \n",
    "    from pandas_datareader import data as pdr\n",
    "    import pandas as pd\n",
    "    import shutil\n",
    "    from collections import OrderedDict\n",
    "    os.chdir(\"/home/smoothjazzuser/videogame-anomoly/MNAD/\")\n",
    "    # get current dir\n",
    "    os.getcwd()\n",
    "    import numpy as np\n",
    "    import sys\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    import torchvision\n",
    "    import torch.nn.init as init\n",
    "    import torch.utils.data as data\n",
    "    import torch.utils.data.dataset as dataset\n",
    "    import torchvision.datasets as dset\n",
    "    import torchvision.transforms as transforms\n",
    "    from torch.autograd import Variable\n",
    "    import torchvision.utils as v_utils\n",
    "    import matplotlib.pyplot as plt\n",
    "    import cv2\n",
    "    import math\n",
    "    from collections import OrderedDict\n",
    "    import copy\n",
    "    import time\n",
    "    from model.utils import DataLoader\n",
    "    from model.final_future_prediction_with_memory_spatial_sumonly_weight_ranking_top1 import *\n",
    "    from model.Reconstruction import *\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import random\n",
    "    import subprocess\n",
    "    import argparse\n",
    "    import matplotlib.pyplot as plt\n",
    "    from time import sleep\n",
    "    import matplotlib.image as mpimg\n",
    "    import shutil\n",
    "    import cv2\n",
    "    from scipy.stats import percentileofscore\n",
    "    import PIL\n",
    "    from joblib import Parallel, delayed\n",
    "    mse = nn.MSELoss(reduction='none')\n",
    "    from torchvision.transforms.functional import gaussian_blur\n",
    "    from model.utils import DataLoader\n",
    "    from utils import *\n",
    "    from model.Reconstruction import *\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCORE</th>\n",
       "      <th>FP_WINDOW</th>\n",
       "      <th>CHANGE_WINDOW</th>\n",
       "      <th>FP</th>\n",
       "      <th>TP</th>\n",
       "      <th>ANOMALIES</th>\n",
       "      <th>ARCHITECHURE</th>\n",
       "      <th>EPOCHS</th>\n",
       "      <th>INFERENCE_LOSS_TYPE</th>\n",
       "      <th>TRAINING_LOSS_TYPE</th>\n",
       "      <th>KERNEL_SIZE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SCORE, FP_WINDOW, CHANGE_WINDOW, FP, TP, ANOMALIES, ARCHITECHURE, EPOCHS, INFERENCE_LOSS_TYPE, TRAINING_LOSS_TYPE, KERNEL_SIZE]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all vars to leep track of. Load previous data if it exists, and extend it where new info is present\n",
    "collumns = ['SCORE','FP_WINDOW', 'CHANGE_WINDOW', 'FP', 'TP', 'ANOMALIES', 'ARCHITECHURE', 'EPOCHS', 'INFERENCE_LOSS_TYPE', 'TRAINING_LOSS_TYPE', \"KERNEL_SIZE\"]\n",
    "if os.path.exists('results.csv'):\n",
    "    df = pd.read_csv('results.csv')\n",
    "    df = df[collumns]\n",
    "    df\n",
    "else:\n",
    "    df = pd.DataFrame(columns=collumns)\n",
    "    df.to_csv('results.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vars \n",
    "loss_sections = 1\n",
    "temp_dir = \"/home/smoothjazzuser/Desktop/ram/\"\n",
    "methodd = \"median_blur\"         #'fourier_blur' # False, 'fourier_blur', 'cummulative_blur', 'median_blur'\n",
    "downscale = True\n",
    "dims = 84\n",
    "channels = 3\n",
    "video = False\n",
    "FP_WINDOW = 60\n",
    "SDEV_SENSITIVITY = 1 # not connected to any vars yet. Scale with the std under the data dist curve... it means something mathematically\n",
    "CHANGE_WINDOW = 10\n",
    "KERNEL_SIZE = 5\n",
    "\n",
    "if not os.path.exists(temp_dir + \"temp/\"):\n",
    "    os.mkdir(temp_dir + \"temp/\")\n",
    "\n",
    "if not os.path.exists(temp_dir + \"cleaned/\"):\n",
    "    os.mkdir(temp_dir + \"cleaned/\")\n",
    "temp_dir = temp_dir + \"temp/\"\n",
    "std_loss_correction = {i:[] for i in range(loss_sections)} #False\n",
    "histories = {}\n",
    "\n",
    "for i in range(3):\n",
    "    histories[f\"d{i}\"] = []\n",
    "    histories[f\"x{i}\"] = []\n",
    "    histories[f\"y{i}\"] = []\n",
    "    histories[f\"t{i}\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "if True:\n",
    "\n",
    "    def normalize_list(list:list):\n",
    "            min_val = min(list)\n",
    "            max_val = max(list)\n",
    "            return [(x-min_val)/(max_val-min_val) for x in list]\n",
    "\n",
    "    def FP_TP (labels, preds):\n",
    "        PREDS = preds.index\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        TOTAL_ANOMALIES = {}#{'anomalous':[i for i in range(len(labels)) if labels[i] == 1]}\n",
    "        #find the places where the labels break sequence\n",
    "        \n",
    "        # find total anomalies\n",
    "        TOTAL_ANOMALIES = []\n",
    "        for i in range(len(labels)):\n",
    "            if i < CHANGE_WINDOW:\n",
    "                continue\n",
    "            num_groups = len(TOTAL_ANOMALIES)\n",
    "            \n",
    "            if labels[i] == 1 and labels[i-1] == 0:\n",
    "                TOTAL_ANOMALIES.append([i])\n",
    "                \n",
    "            elif labels[i] == 1 and labels[i-1] == 1:\n",
    "                if num_groups == 0:\n",
    "                    TOTAL_ANOMALIES.append([i])\n",
    "                else:\n",
    "                    TOTAL_ANOMALIES[num_groups - 1].append(i)\n",
    "\n",
    "        # find TP and FP\n",
    "        anomally_assignments = {index:[] for index, i in enumerate(TOTAL_ANOMALIES)}\n",
    "        irrelevent = []\n",
    "        for pred_index in PREDS:\n",
    "            if pred_index < CHANGE_WINDOW:\n",
    "                continue\n",
    "\n",
    "            n_within_window = [l for l in range(pred_index-FP_WINDOW, pred_index+FP_WINDOW + 1)]\n",
    "\n",
    "            # if any value in n_within_window is also in TOTAL_ANOMALIES\n",
    "            valid_pred = False\n",
    "\n",
    "            for win in n_within_window:\n",
    "                for bin_values in TOTAL_ANOMALIES:\n",
    "                    if win in bin_values:\n",
    "                        valid_pred = True\n",
    "                        anomally_assignments[TOTAL_ANOMALIES.index(bin_values)].append(1)\n",
    "                        break\n",
    "            if not valid_pred:\n",
    "                false_preds_window =  list(range(pred_index-FP_WINDOW, pred_index+FP_WINDOW + 1))\n",
    "                if len(set(false_preds_window).intersection(irrelevent)) == 0:\n",
    "                    FP += 1\n",
    "                    irrelevent.extend(false_preds_window)\n",
    "                else:\n",
    "                    # merge the two lists\n",
    "                    irrelevent = list(set(false_preds_window).union(irrelevent))\n",
    "                    \n",
    "        for key, value in anomally_assignments.items():\n",
    "            if sum(value) > 0:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "\n",
    "        return f\"TP_{TP}_FP_{FP}_FN_{FN}_ANOMALIES_{len(TOTAL_ANOMALIES)}\", (TP, FP, FN, len(TOTAL_ANOMALIES), TOTAL_ANOMALIES, anomally_assignments)\n",
    "\n",
    "    def load_(filename):\n",
    "        i = plt.imread(filename)\n",
    "        return  i\n",
    "\n",
    "    def load_frames(i, h, w): \n",
    "        if os.path.exists(f\"/tmp/{i}.jpg\"):\n",
    "            return np.uint8(cv2.resize(imageio.imread(f\"/tmp/{i}.jpg\"), (h,w)))\n",
    "        else:\n",
    "            return np.uint8(cv2.resize(imageio.imread(f\"/tmp/{i}.png\"), (h,w)))\n",
    "\n",
    "    def normalize_array(array:np.ndarray):\n",
    "        min_val = np.min(array)\n",
    "        max_val = np.max(array)\n",
    "        return (array-min_val)/(max_val-min_val)\n",
    "\n",
    "    def split_list(list:list, n:int):\n",
    "        return [list[i:i+n] for i in range(0, len(list), n)]\n",
    "\n",
    "    def plot_images(i, x, diffs, ground_truth, label_plot, preds, points):\n",
    "            window = 50\n",
    "            if i > 50: window = i - 50\n",
    "            else: window = 0\n",
    "            \n",
    "            #plot preds\n",
    "            fig = plt.figure()\n",
    "            fig.set_size_inches(10, 10, forward=True)\n",
    "            ax = fig.add_subplot(2,2,1)\n",
    "            ax.set_title(f\"prediction {window}:{i}\")\n",
    "            ax.imshow(preds, cmap='gray') \n",
    "\n",
    "            #plot diffs\n",
    "            ax = fig.add_subplot(2,2,2)\n",
    "            ax.set_title(f\"diff {window}:{i}\")\n",
    "            ax.imshow(diffs, cmap='gray')\n",
    "\n",
    "            #plot ground_truth\n",
    "            ax = fig.add_subplot(2,2,4)\n",
    "            ax.set_title(f\"ground truth {window}:{i}\")\n",
    "            ax.imshow(ground_truth, cmap='gray')\n",
    "\n",
    "            #plot anomaly score\n",
    "            ax = fig.add_subplot(2,2,3)\n",
    "            ax.set_title(f\"anomaly score {window}:{i}\")\n",
    "            ax.plot(np.arange(len(x[window:i])), x[window:i], color='black', label='anomaly score', linewidth=0.6)\n",
    "            ax.scatter(np.arange(len(x[window:i])), x[window:i], color='black', label='anomaly score', s=4)\n",
    "            ax.scatter(np.arange(len(label_plot['x'][window:i])), label_plot['y'][window:i], color='red', marker='x', s=4)\n",
    "\n",
    "            # find all search.points that are in the window\n",
    "            matching = [(i,points[x]) for i, x in enumerate(np.arange(window, i)) if x in points.index]\n",
    "            # plot these points with y-values = data y values\n",
    "            ax.scatter([x[0] for x in matching], [x[1] for x in matching], label='Flagged points', color='purple', s=14, alpha=0.5, marker='o')\n",
    "\n",
    "\n",
    "            #ax.legend([metric])\n",
    "            fig.tight_layout()\n",
    "            fig.subplots_adjust(wspace=0, hspace=0)\n",
    "            #set ax to show 50 x values\n",
    "            ax.set_xlim(0, 50)\n",
    "            ax.set_ylim(0, 1)\n",
    "            fig.savefig(f\"/tmp/{i}.jpg\", bbox_inches='tight', pad_inches=0)\n",
    "            \n",
    "            fig.clear()\n",
    "            plt.close(fig)\n",
    "            del ax\n",
    "            del window\n",
    "            del fig\n",
    "            return None\n",
    "\n",
    "    def percentile  (array, percentile):\n",
    "        percentile = np.squeeze(percentile)\n",
    "        array = np.squeeze(array)\n",
    "        return np.percentile(array, percentile)\n",
    "\n",
    "    def copy_frames(folder, destination, keep_every):\n",
    "                files = sorted(glob(os.path.join(folder, '*.jpg')), key=lambda x: int(x.split('/')[-1].split('.')[0])) + sorted(glob(os.path.join(folder, '*.png')), key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "                dest = os.path.join(destination, folder.split('/')[-1])\n",
    "                for i, file in enumerate(files):\n",
    "                    if i % keep_every == 0:\n",
    "                        shutil.copy(file, os.path.join(dest, os.path.basename(file)))\n",
    "                # rename all files in the folder to be sequential, starting at 0\n",
    "                files = sorted(glob(os.path.join(dest, '*.jpg')), key=lambda x: int(x.split('/')[-1].split('.')[0])) + sorted(glob(os.path.join(dest, '*.png')), key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "                for i, file in enumerate(files):\n",
    "                    os.rename(file, os.path.join(dest, str(i).zfill(3)+'.jpg'))\n",
    "\n",
    "                # remove frames until len(files) is a multiple of 5\n",
    "                files = sorted(glob(os.path.join(dest, '*.jpg')), key=lambda x: int(x.split('/')[-1].split('.')[0])) + sorted(glob(os.path.join(dest, '*.png')), key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "                while len(files) % 1 != 0:\n",
    "                    os.remove(files[-1])\n",
    "                    files = sorted(glob(os.path.join(dest, '*.jpg')), key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "                del files, dest, folder, destination, keep_every\n",
    "\n",
    "    def bl (x, y, ix, id):\n",
    "        x = cv2.GaussianBlur(x, (5,5), 0)\n",
    "        y = cv2.GaussianBlur(y, (5,5), 0)\n",
    "\n",
    "        x = np.squeeze(x)\n",
    "        y = np.squeeze(y)\n",
    "        \n",
    "        x = np.fft.fft2(x)\n",
    "        y = np.fft.fft2(y)\n",
    "        x = np.fft.fftshift(x)\n",
    "        y = np.fft.fftshift(y)\n",
    "        #set all values below 0.1 to 0\n",
    "        x_h = np.percentile(np.abs(x), 100)\n",
    "        y_h = np.percentile(np.abs(y), 100)\n",
    "        x_l = np.percentile(np.abs(x), 90)\n",
    "        y_l = np.percentile(np.abs(y), 90)\n",
    "        x = np.where((np.abs(x) > x_h) &  (np.abs(x) < x_l), 0, x)\n",
    "        y = np.where((np.abs(y) > y_h) &  (np.abs(y) < y_l), 0, y)\n",
    "        x = np.fft.ifftshift(x)\n",
    "        y = np.fft.ifftshift(y)\n",
    "        x = np.fft.ifft2(x)\n",
    "        y = np.fft.ifft2(y)\n",
    "        \n",
    "        x = np.abs(x)\n",
    "        y = np.abs(y)\n",
    "\n",
    "        x = cv2.GaussianBlur(x, (5,5), 0)\n",
    "        y = cv2.GaussianBlur(y, (5,5), 0)\n",
    "\n",
    "        d = np.abs(x-y)\n",
    "        for _ in range(10):\n",
    "            d = cv2.GaussianBlur(d, (11,11), 0)\n",
    "\n",
    "        histories[str(id)].append(d)\n",
    "        if len(histories[str(id)]) > 25: \n",
    "            histories[str(id)].pop(0)\n",
    "        d = np.abs(d - np.mean(np.array(histories[str(id)]) , axis=0)) + 1e-9\n",
    "        # for _ in range(10):\n",
    "        #     d = cv2.GaussianBlur(d, (11,11), 0)\n",
    "        thresh_d = np.percentile(d, 90)\n",
    "        histories['t'+str(id)[1]].append(thresh_d)\n",
    "        if len(histories['t'+str(id)[1]]) > 10:\n",
    "            histories['t'+str(id)[1]].pop(0)\n",
    "        d = np.where(d < np.mean(histories['t'+str(id)[1]]), 0, d)\n",
    "\n",
    "        return d\n",
    "\n",
    "    def cb(x, y, ix, id):\n",
    "        global histories\n",
    "        \n",
    "        x = np.squeeze(x)\n",
    "        y = np.squeeze(y)\n",
    "\n",
    "        d = np.abs(x-y) + 1e-9\n",
    "        \n",
    "        for i in range(4):\n",
    "            d = cv2.GaussianBlur(d, (11,11), 0)\n",
    "\n",
    "        if type(histories[str(id)]) == list:\n",
    "            histories[str(id)] = d\n",
    "\n",
    "        d = np.abs(d - histories[str(id)]) + 1e-9\n",
    "\n",
    "        histories[str(id)] = (histories[str(id)] + d/10) / (1 + 1/10)\n",
    "        #d = np.where(d < histories[str(id)], d, histories[str(id)])\n",
    "\n",
    "        return d\n",
    "\n",
    "    def mb(x, y, ix, id):\n",
    "        x = np.squeeze(x)\n",
    "        y = np.squeeze(y)\n",
    "        x = x * 255\n",
    "        y = y * 255\n",
    "        x = x.astype(np.uint8)\n",
    "        y = y.astype(np.uint8)\n",
    "        for i in range(1):\n",
    "            x = cv2.medianBlur(x, KERNEL_SIZE)\n",
    "            y = cv2.medianBlur(y, KERNEL_SIZE)\n",
    "        d = np.abs(np.subtract(x,y), dtype=np.float32)**2\n",
    "        return d\n",
    "\n",
    "    def gb(x, y, ix, id):\n",
    "        x = np.squeeze(x)\n",
    "        y = np.squeeze(y)\n",
    "        x = x * 255\n",
    "        y = y * 255\n",
    "        x = x.astype(np.uint8)\n",
    "        y = y.astype(np.uint8)\n",
    "        for i in range(1):\n",
    "            x = cv2.GaussianBlur(x, (KERNEL_SIZE,KERNEL_SIZE), 0)\n",
    "            y = cv2.GaussianBlur(y, (KERNEL_SIZE,KERNEL_SIZE), 0)\n",
    "        d = np.abs(np.subtract(x,y), dtype=np.float32)**2\n",
    "        return d\n",
    "    \n",
    "    def mse(x, y, ix, id):\n",
    "        x = np.squeeze(x)\n",
    "        y = np.squeeze(y)\n",
    "        d = np.abs(np.subtract(x,y), dtype=np.float32)**2\n",
    "        return d\n",
    "\n",
    "    def loss_func_mse(x, y, fft=True, colors=True, method=methodd, std_details=True, diff_ = True):\n",
    "        global loss_sections, std_loss_correction, histories\n",
    "        losses = {'x':[], 'y':[]}\n",
    "        x.transpose_(1,3)\n",
    "        y.transpose_(1,3)\n",
    "        x = x.cpu().detach().numpy()\n",
    "        y = y.cpu().detach().numpy()\n",
    "        x = np.squeeze(x)\n",
    "        y = np.squeeze(y)\n",
    "        x = cv2.resize(x, (dims, dims))\n",
    "        y = cv2.resize(y, (dims, dims))\n",
    "\n",
    "        if methodd:\n",
    "            d = np.zeros(x.shape)\n",
    "            if methodd == 'fourier_blur':\n",
    "                x = x.astype(np.float64)\n",
    "                y = y.astype(np.float64)\n",
    "                for i in range(x.shape[0]): # we are using color images\n",
    "                    d[:,:,i] = bl(x[i,:,:], y[i,:,:], f'x{i}', f'd{i}')\n",
    "            \n",
    "            elif methodd == 'cummulative_blur':\n",
    "                x = x.astype(np.float64)\n",
    "                y = y.astype(np.float64)\n",
    "                for i in range(x.shape[0]):\n",
    "                    d[:,:,i] = cb(x[i,:,:], y[i,:,:], f'x{i}', f'd{i}')\n",
    "\n",
    "            elif methodd == 'median_blur' or methodd == 'MSE_BLUR':\n",
    "                for i in range(channels):\n",
    "                    d[:,:,i] = mb(x[:,:,i], y[:,:,i], f'x{i}', f'd{i}')\n",
    "\n",
    "            elif methodd == 'gaussian_blur' or methodd == 'G_BLUR':\n",
    "                for i in range(channels):\n",
    "                    d[:,:,i] = gb(x[:,:,i], y[:,:,i], f'x{i}', f'd{i}')\n",
    "\n",
    "            else:\n",
    "                for i in range(channels):\n",
    "                    d[:,:,i] = mse(x[:,:,i], y[:,:,i], f'x{i}', f'd{i}')\n",
    "\n",
    "            loss = d\n",
    "\n",
    "        else:\n",
    "            #loss = F.mse_loss(x, y)\n",
    "            loss = np.abs(x-y)**2\n",
    "\n",
    "        return loss#**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class anomaly():\n",
    "  \n",
    "    def __init__(self, method, metric, std_correction, dataset):\n",
    "        self.method, self.metric, self.std_correction, self.dataset = method, metric, std_correction, dataset\n",
    "        self.model = False\n",
    "        self.m_items = False\n",
    "   \n",
    "    def swap_test_folders(self, inference='01'):\n",
    "        os.chdir(\"/home/smoothjazzuser/videogame-anomoly/\")\n",
    "        self.inference = inference\n",
    "        #check if frames exists:\n",
    "        if os.path.exists('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames'):\n",
    "            !rm -r '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames'\n",
    "\n",
    "        if inference == 'all':\n",
    "            !ln -s '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/symlink_loc/' '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames'\n",
    "        else:\n",
    "            if not os.path.exists('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames'):\n",
    "                !mkdir '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames/'\n",
    "            !ln -s '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/symlink_loc/'$inference '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames/'$inference\n",
    "        ##############################################################################\n",
    "        frame_labels_bugs = np.load('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/symlink_loc/GTall/frame_labels_bugs.npy', allow_pickle=True)\n",
    "        # expand dims and save again\n",
    "        #frame_labels_bugs = np.expand_dims(frame_labels_bugs, axis=0)\n",
    "        frame_labels_bugs[0]\n",
    "        #np.save('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames/frame_labels_bugs.npy', frame_labels_bugs)\n",
    "        if True:\n",
    "            frame_labels_bugs = np.load('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/symlink_loc/GTall/frame_labels_bugs.npy', allow_pickle=True)\n",
    "\n",
    "            # list directories in /home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames/ using glob\n",
    "            # glob returns a list of all files in the directory\n",
    "            dd = glob('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames/*')\n",
    "            dd.sort()\n",
    "            directories = [x.split('/')[-1] for x in dd if x.split('/')[-1].isdigit()]\n",
    "            directories.sort()\n",
    "            directories = {d:len(glob('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames/' + d + '/*.jpg') + glob('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames/' + d + '/*.png')) for d in directories if os.path.exists('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames/' + d)}\n",
    "            directories = OrderedDict(sorted(directories.items(), key=lambda x: int(x[0])))\n",
    "            print(list(directories))\n",
    "            iterate = 0\n",
    "            for folder, num_frames in directories.items():\n",
    "                if not os.path.exists('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames/' + folder):\n",
    "                    !mkdir '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/testing/frames/'$folder\n",
    "\n",
    "                    print(folder, num_frames)\n",
    "                if not os.path.exists('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/symlink_loc/GT' + folder + '/frame_labels_bugs.npy'):\n",
    "                    !mkdir '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/symlink_loc/GT'$folder\n",
    "                    # seperate the labels for each folder from the frame_labels_bugs\n",
    "                    print(folder, num_frames)\n",
    "                    labels = frame_labels_bugs[0][iterate:iterate+int(num_frames)]\n",
    "                    iterate += num_frames\n",
    "                    np.save('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/symlink_loc/GT' + folder + '/frame_labels_bugs.npy', np.array(np.expand_dims(np.array(labels), axis=0)))\n",
    "        if os.path.exists('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/frame_labels_bugs.npy'):\n",
    "            !rm '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/frame_labels_bugs.npy'\n",
    "        if inference == 'all':\n",
    "            !ln -s '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/symlink_loc/GTall/frame_labels_bugs.npy' '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/frame_labels_bugs.npy'\n",
    "        else:\n",
    "            !ln -s '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/symlink_loc/GT'$inference'/frame_labels_bugs.npy' '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/frame_labels_bugs.npy'\n",
    "\n",
    "        test = np.load('/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/frame_labels_bugs.npy', allow_pickle=True)\n",
    "        print(test.shape)\n",
    "        (sum(directories.values()))\n",
    "\n",
    "    def Evaluate(self, c=channels, loss_sections=1, method='blur', method_ = 'recon', dataset_type='bugs', image_size=dims, metric_only=False):\n",
    "        os.chdir(\"/home/smoothjazzuser/videogame-anomoly/MNAD/\")\n",
    "        std_loss_correction = {i:[] for i in range(loss_sections)} #False\n",
    "\n",
    "        args = {'gpus':'','batch_size':5,'test_batch_size':1,'h':image_size,'w':image_size,'c':c,'method':method_,'t_length':1 if method_=='recon' else 5,'fdim':512,'mdim':512, 'msize':10,'alpha':0.6,'th':0.0000,'num_workers':8,'num_workers_test':1,'dataset_type':'bugs','dataset_path':'./dataset','model_dir':f'./exp/{dataset_type}/{method_}/log/model.pth','m_items_dir':f'./exp/{dataset_type}/{method_}/log/keys.pt'}\n",
    "        \n",
    "\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "        gpus = \"0\"\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpus\n",
    "        torch.backends.cudnn.enabled = True # make sure to use cudnn for computational performance\n",
    "        test_folder = args['dataset_path']+\"/\"+args['dataset_type']+\"/testing/frames\"\n",
    "        test_dataset = DataLoader(test_folder, transforms.Compose([transforms.ToTensor(),]), resize_height=args['h'], resize_width=args['w'], time_step=args['t_length']-1)\n",
    "        test_size = len(test_dataset)\n",
    "        print(\"The number of test data is %d\" % test_size)\n",
    "        test_batch = data.DataLoader(test_dataset, batch_size = args['test_batch_size'], shuffle=False, num_workers=args['num_workers_test'], drop_last=False)\n",
    "\n",
    "        ###############################\n",
    "        histories = {}\n",
    "        for i in range(args['c']):\n",
    "            histories[f\"d{i}\"] = []\n",
    "            histories[f\"x{i}\"] = []\n",
    "            histories[f\"y{i}\"] = []\n",
    "            histories[f\"t{i}\"] = []\n",
    "\n",
    "            \n",
    "        #######################################################\n",
    "        # Loading the trained model\n",
    "        if not self.model: \n",
    "            model = torch.load(args['model_dir'])\n",
    "            model.cuda()\n",
    "            m_items = torch.load(args['m_items_dir'])\n",
    "        else:\n",
    "            model = self.model\n",
    "            m_items = self.m_items\n",
    "        labels = np.load('./data/frame_labels_'+args['dataset_type']+'.npy')\n",
    "\n",
    "        videos = OrderedDict()\n",
    "\n",
    "        videos_list = sorted(glob(os.path.join(test_folder, '*')), key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "        for video in videos_list:\n",
    "            video_name = video.split('/')[-1]\n",
    "            videos[video_name] = {}\n",
    "            videos[video_name]['path'] = video\n",
    "            videos[video_name]['frame'] = sorted(glob(os.path.join(video, '*.jpg')) + glob(os.path.join(video, '*.png')), key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "            #videos[video_name]['frame'].sort()\n",
    "            videos[video_name]['length'] = len(videos[video_name]['frame'])\n",
    "\n",
    "        labels_list = []\n",
    "        label_length = 0\n",
    "        psnr_list = {}\n",
    "        feature_distance_list = {}\n",
    "\n",
    "        print('Evaluation of', args['dataset_type'])\n",
    "\n",
    "        # Setting for video anomaly detection\n",
    "        for video in sorted(videos_list, key=lambda x: int(x.split('/')[-1].split('.')[0])):\n",
    "        #for video in videos_list:\n",
    "            video_name = video.split('/')[-1]\n",
    "            if args['method'] == 'pred':\n",
    "                labels_list = np.append(labels_list, labels[0][4+label_length:videos[video_name]['length']+label_length])\n",
    "            else:\n",
    "                labels_list = np.append(labels_list, labels[0][label_length:videos[video_name]['length']+label_length])\n",
    "            label_length += videos[video_name]['length']\n",
    "            psnr_list[video_name] = []\n",
    "            feature_distance_list[video_name] = []\n",
    "\n",
    "        label_length = 0\n",
    "        video_num = 0\n",
    "        label_length += videos[videos_list[video_num].split('/')[-1]]['length']\n",
    "        m_items_test = m_items.clone()\n",
    "\n",
    "        model.eval()\n",
    "        kkk=0\n",
    "        ccc = int(test_size/args['test_batch_size'])\n",
    "        if not metric_only: diffs = []\n",
    "        if not metric_only: preds = []\n",
    "        if not metric_only: ground_truths = []\n",
    "        label_list = np.load(f\"/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/frame_labels_{args['dataset_type']}.npy\", allow_pickle=True).tolist()[0]\n",
    "        loss_hist = {i:[] for i in range(loss_sections)}\n",
    "        for k,(imgs) in enumerate(test_batch):\n",
    "            \n",
    "            if k == label_length:\n",
    "                video_num += 1\n",
    "                label_length += videos[videos_list[video_num].split('/')[-1]]['length']\n",
    "\n",
    "            imgs = Variable(imgs).cuda()\n",
    "            outputs, feas, updated_feas, m_items_test, softmax_score_query, softmax_score_memory, compactness_loss = model.forward(imgs, m_items_test, False)\n",
    "            mse_imgs = np.sum(loss_func_mse(outputs, imgs))#.item()\n",
    "            mse_feas = compactness_loss.item()\n",
    "            if not metric_only:\n",
    "                diff = loss_func_mse(outputs, imgs)\n",
    "                #diff = normalize_array(diff)\n",
    "                diffs.append(diff)\n",
    "\n",
    "                pred = (outputs[0].detach().cpu().numpy()+1)/2\n",
    "                pred = pred.transpose(1,2,0)\n",
    "                preds.append(pred)\n",
    "\n",
    "            kkk+=1\n",
    "\n",
    "            psnr_list[videos_list[video_num].split('/')[-1]].append(psnr(mse_imgs))\n",
    "            feature_distance_list[videos_list[video_num].split('/')[-1]].append(mse_feas)\n",
    "            print(f\"pairs pred: {round(100*kkk/ccc,3)}%, mse_feas:{mse_feas}\", end = \"\\r\")\n",
    "\n",
    "\n",
    "        # Measuring the abnormality score and the AUC\n",
    "        anomaly_score_total_list = []\n",
    "        for video in sorted(videos_list):\n",
    "            video_name = video.split('/')[-1]\n",
    "            anomaly_score_total_list += score_sum(anomaly_score_list(psnr_list[video_name]), \n",
    "                                            anomaly_score_list_inv(feature_distance_list[video_name]), args['alpha'])\n",
    "\n",
    "        anomaly_score_total_list = np.asarray(anomaly_score_total_list)\n",
    "\n",
    "        if not os.path.exists(f\"{temp_dir}{args['dataset_type']}\"):\n",
    "            os.mkdir(f\"{temp_dir}{args['dataset_type']}\")\n",
    "        if not os.path.exists(f\"{temp_dir}{args['dataset_type']}/{args['method']}\"):\n",
    "            os.mkdir(f\"{temp_dir}{args['dataset_type']}/{args['method']}\")\n",
    "        if not os.path.exists(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log\"):\n",
    "            os.mkdir(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log\")\n",
    "\n",
    "        if os.path.exists(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/diffs/\"):\n",
    "            l = f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/diffs/\"\n",
    "            shutil.rmtree(l)\n",
    "        if os.path.exists(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/preds/\"):\n",
    "            l = f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/preds/\"\n",
    "            shutil.rmtree(l)\n",
    "        if os.path.exists(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/ground_truths/\"):\n",
    "            l = f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/ground_truths/\"\n",
    "            shutil.rmtree(l)\n",
    "\n",
    "        if not os.path.exists(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/diffs/\"):\n",
    "            os.makedirs(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/diffs/\")\n",
    "        if not os.path.exists(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/preds/\"):\n",
    "            os.makedirs(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/preds/\")\n",
    "\n",
    "        np.save(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/anomaly_score_total_list.npy\", anomaly_score_total_list)\n",
    "        np.save(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/anomaly_score_total_list.npy\", anomaly_score_total_list)\n",
    "        np.save(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/psnr_list.npy\", psnr_list)\n",
    "        np.save(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/feature_distance_list.npy\", feature_distance_list)\n",
    "        #np.save(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/auc.npy\", accuracy)\n",
    "        np.save(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/labels_list.npy\", labels_list)\n",
    "\n",
    "        \n",
    "        if not metric_only:\n",
    "            for (i,img) in enumerate(preds): \n",
    "                #use pillow to save the image in grayscale\n",
    "                if args['c'] == 1:\n",
    "                    img = img*255\n",
    "                    img = img.astype(np.uint8)\n",
    "                    img = np.squeeze(img)\n",
    "                    img = PIL.Image.fromarray(img, 'L')\n",
    "                    img.save(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/preds/{i}.jpg\")\n",
    "                else:\n",
    "                    img = img*255\n",
    "                    img = img.astype(np.uint8)\n",
    "                    img = PIL.Image.fromarray(img, 'RGB')\n",
    "                    img.save(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/preds/{i}.jpg\")\n",
    "            if methodd == False or methodd != False:\n",
    "                for (i,img) in enumerate(diffs): \n",
    "                    if args['c'] == 1:\n",
    "                        img = img*255\n",
    "                        img = img.astype(np.uint8)\n",
    "                        img = np.squeeze(img)\n",
    "                        img = PIL.Image.fromarray(img, 'L')\n",
    "                        img.save(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/diffs/{i}.jpg\")\n",
    "                    else:\n",
    "                        img = img*255\n",
    "                        img = img.astype(np.uint8)\n",
    "                        img = PIL.Image.fromarray(img, 'RGB')\n",
    "                        img.save(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/diffs/{i}.jpg\")\n",
    "            else:\n",
    "                shutil.rmtree(f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/diffs/\")\n",
    "                shutil.copytree(f\"{temp_dir}cleaned/\", f\"{temp_dir}{args['dataset_type']}/{args['method']}/log/diffs/\")\n",
    "\n",
    "\n",
    "        print('The result of ', args['dataset_type'])\n",
    "\n",
    "    def rm_train_music_copy(self):\n",
    "        if not os.path.exists('/tmp/empty'):\n",
    "            os.makedirs('/tmp/empty')\n",
    "        !rsync -a --delete /tmp/empty/   /home/smoothjazzuser/VQ-VAE-Search-main/mel_specs_music/train/cl/\n",
    "        !rsync -a --delete /tmp/empty/   /home/smoothjazzuser/VQ-VAE-Search-main/mel_specs_music/test/cl/\n",
    "   \n",
    "    def cut_dataset(self, keep_every=10, ram=True, source_path='/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/training/full', destination = '/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/training/frames', ):\n",
    "        os.chdir(\"/home/smoothjazzuser/videogame-anomoly/\")\n",
    "        self.ls_destination = destination\n",
    "        self.source_path = source_path\n",
    "        self.destination = destination\n",
    "        self.keep_every = keep_every\n",
    "        self.ram = ram\n",
    "        if not os.path.exists('/tmp/empty'):\n",
    "            os.makedirs('/tmp/empty')\n",
    "        if self.ram:\n",
    "            if not os.path.exists('/home/smoothjazzuser/Desktop/ram/frames/'):\n",
    "                os.makedirs('/home/smoothjazzuser/Desktop/ram/frames/')\n",
    "            self.destination = '/home/smoothjazzuser/Desktop/ram/frames'\n",
    "            !rsync -a --delete /tmp/empty/   /home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/training/frames/\n",
    "            !rsync -a --delete /tmp/empty/   /home/smoothjazzuser/Desktop/ram/frames/\n",
    "            !rm -rf /home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/training/frames\n",
    "        else: \n",
    "            !rsync -a --delete /tmp/empty/   /home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/training/frames/\n",
    "            #del /frames/ folder\n",
    "            !rm -rf /home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/training/frames\n",
    "            #create new /frames/ folder\n",
    "            !mkdir /home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/training/frames\n",
    "            print('Saving to RAM')\n",
    "        !mkdir -p /tmp/empty\n",
    "\n",
    "        self.folders_list = sorted(glob(os.path.join(self.source_path, '*')))\n",
    "        print(len(self.folders_list))\n",
    "\n",
    "        # create all desitnation folders, if they don't exist\n",
    "        for folder in self.folders_list:\n",
    "            if not os.path.exists(os.path.join(self.destination, folder.split('/')[-1])):\n",
    "                os.makedirs(os.path.join(self.destination, folder.split('/')[-1]))\n",
    "\n",
    "        \n",
    "\n",
    "        c = Parallel(n_jobs=12)(delayed(copy_frames)(folder, self.destination, self.keep_every) for folder in self.folders_list)\n",
    "        del c\n",
    "        if ram:\n",
    "            !ln -s /home/smoothjazzuser/Desktop/ram/frames/ /home/smoothjazzuser/videogame-anomoly/MNAD/dataset/bugs/training/\n",
    "\n",
    "   \n",
    "    def analysis(self, video=True, metric_only=False):\n",
    "        # change working directory in jupyter notebook\n",
    "        os.chdir(\"/home/smoothjazzuser/videogame-anomoly/\")\n",
    "        self.anomaly_score_total_list = np.load(f\"{temp_dir}{self.dataset}/{self.method}/log/anomaly_score_total_list.npy\", allow_pickle=True)\n",
    "        self.feature_distance_list = np.load(f\"{temp_dir}{self.dataset}/{self.method}/log/feature_distance_list.npy\", allow_pickle=True)\n",
    "        self.psnr_list = np.load(f\"{temp_dir}{self.dataset}/{self.method}/log/psnr_list.npy\", allow_pickle=True)\n",
    "        self.label_list = np.load(f\"/home/smoothjazzuser/videogame-anomoly/MNAD/dataset/frame_labels_{self.dataset}.npy\", allow_pickle=True).tolist()[0]\n",
    "        self.x = self.anomaly_score_total_list\n",
    "        self.name = self.key = list(np.array(self.psnr_list).reshape(1).tolist()[0].keys())[0]\n",
    "        print('len x',len(self.x),\"len y\",len(self.label_list), \"key:\", self.key)\n",
    "        ############################################################################################################\n",
    "        self.psnr = normalize_list(np.array(self.psnr_list).reshape(1).tolist()[0][self.name])\n",
    "        self.feature_distance = normalize_list(self.feature_distance_list.tolist()[self.name])\n",
    "        self.adjustment = 4 if self.method == 'pred' else 0\n",
    "        self.x = self.x[self.adjustment:]\n",
    "        self.label_list = self.label_list[self.adjustment:]\n",
    "        self.psnr = self.psnr[self.adjustment:]\n",
    "        self.feature_distance = self.feature_distance[self.adjustment:]\n",
    "        self.x_shape = self.x.shape[0]\n",
    "        self.start = 0\n",
    "        self.end = int(self.x_shape)\n",
    "        ############################################################################################################\n",
    "        if self.metric == 'anomaly_score':\n",
    "            self.x = self.x[self.start:self.end]\n",
    "        elif self.metric == 'feature_distance':\n",
    "            self.x = self.feature_distance[self.start:self.end]\n",
    "        elif self.metric == 'psnr':\n",
    "            self.x = self.psnr[self.start:self.end]\n",
    "\n",
    "        self.label_list = self.label_list[self.start:self.end]\n",
    "        self.label_points = {'x':[], 'y':[]}\n",
    "        self.label_plot = {'x':[], 'y':[]}\n",
    "        for i in range(len(self.label_list)):\n",
    "            if self.label_list[i] == 1:\n",
    "                self.label_points['x'].append(i)\n",
    "                self.label_points['y'].append(self.x[i])\n",
    "                \n",
    "                self.label_plot['y'].append(self.x[i])\n",
    "            else:\n",
    "                self.label_plot['y'].append(0)\n",
    "            self.label_plot['x'].append(i)\n",
    "\n",
    "        stock, start, end, x, y = False, False, False, self.x, self.label_points\n",
    "        self.data = x\n",
    "        self.data = self.normalize_data(self.data)\n",
    "        self.label_points = y\n",
    "        self.data = pd.Series(self.data)\n",
    "        self.label_points = pd.DataFrame.from_dict(self.label_points, orient='index')\n",
    "        self.label_points = self.label_points.transpose() \n",
    "        self.zeros = pd.DataFrame(np.zeros(self.data.shape[0]), columns=['zeros'])\n",
    "        ############################################################################################################\n",
    "        \n",
    "        self.TPTP = False\n",
    "        self.plot(window=CHANGE_WINDOW , alpha=0.5,  plots = ['bet_std_mean', 'std', 'data','mean'])\n",
    "\n",
    "        \n",
    "        if not metric_only:\n",
    "            os.makedirs(f\"home/smoothjazzuser/Desktop/output\", exist_ok=True)\n",
    "            self.show_plot(save=True, score_str=self.TPTP)\n",
    "        else:\n",
    "            video = False\n",
    "\n",
    "        \n",
    "        if video:\n",
    "            self.r = int((len(self.label_list)-1)/1)\n",
    "            self.preds = sorted(glob(f\"{temp_dir}{self.dataset}/{self.method}/log/preds/*.jpg\"), key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "            self.diffs = sorted(glob(f\"{temp_dir}{self.dataset}/{self.method}/log/diffs/*.jpg\"), key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "            self.ground_truth = []\n",
    "            self.folders = sorted(glob(f\"/home/smoothjazzuser/Desktop/videogame-anomoly/MNAD/dataset/{self.dataset}/testing/frames/*/\"), key=lambda x: int(x.split('/')[-2]))\n",
    "            self.files = [sorted(glob(f\"{file}*.jpg\") + glob(f\"{file}*.png\"), key=lambda x: int(x.split('/')[-1].split('.')[0])) for file in self.folders]\n",
    "            for folder in sorted(glob(f\"/home/smoothjazzuser/Desktop/videogame-anomoly/MNAD/dataset/{self.dataset}/testing/frames/*/\"), key=lambda x: int(x.split('/')[-2])):\n",
    "                for file in sorted(glob(f\"{folder}*.jpg\") + glob(f\"{folder}*.png\"), key=lambda x: int(x.split('/')[-1].split('.')[0])):\n",
    "                    self.ground_truth.append(plt.imread(file))\n",
    "\n",
    "            self.preds = [plt.imread(pred) for pred in self.preds]\n",
    "            #self.preds = Parallel(n_jobs=12, backend='multiprocessing')(delayed(plt.imread)(pred) for pred in self.preds)\n",
    "            self.diffs = [plt.imread(diff) for diff in self.diffs]\n",
    "            #self.diffs = Parallel(n_jobs=12, backend='multiprocessing')(delayed(plt.imread)(diff) for diff in self.diffs)\n",
    "            !rm -rf /tmp/*.jpg\n",
    "            !rm -rf /tmp/*.png\n",
    "            !rm -rf /tmp/*.svg\n",
    "            #split r into 10 chunks\n",
    "            self.rr = list(range(self.r))\n",
    "            print(f\"rr: {len(self.rr)}, preds: {len(self.preds)}, diffs: {len(self.diffs)}, ground_truth: {len(self.ground_truth)}\")\n",
    "            a = Parallel(n_jobs=12, backend='multiprocessing')(delayed(plot_images)(i, self.x, self.diffs[i], self.ground_truth[i], self.label_plot, self.preds[i], self.points) for i in self.rr[0:int(len(self.rr)/2)])\n",
    "            del a\n",
    "            gc.collect()\n",
    "            a = Parallel(n_jobs=12, backend='multiprocessing')(delayed(plot_images)(i, self.x, self.diffs[i], self.ground_truth[i], self.label_plot, self.preds[i], self.points) for i in self.rr[int(len(self.rr)/2):-1])\n",
    "            del a\n",
    "            gc.collect()\n",
    "            del self.preds\n",
    "            del self.diffs\n",
    "            del self.ground_truth\n",
    "            gc.collect()\n",
    "            self.a = imageio.imread(f\"/tmp/0.jpg\").shape\n",
    "            self.h,self.w = self.a[0], self.a[1]\n",
    "            self.frames = Parallel(n_jobs=12, backend='multiprocessing')(delayed(load_frames)(i, self.h, self.w) for i in tqdm(range(self.r-1))) \n",
    "            !rm -rf /tmp/*.jpg\n",
    "            !rm -rf /tmp/*.svg\n",
    "            !rm -rf /tmp/*.png\n",
    "            #delete all variables except frames\n",
    "            gc.collect()\n",
    "            if not os.path.exists(f\"{temp_dir.replace('ram/temp', 'output')}\"):\n",
    "                os.mkdir(f\"{temp_dir.replace('ram/temp', 'output')}\")\n",
    "            imageio.mimsave(f\"{temp_dir.replace('ram/temp', 'output')}folder-{self.name}_{methodd}_{self.TPTP}.mp4\", self.frames, fps=10)\n",
    "            del self.frames\n",
    "            gc.collect()\n",
    "        else:\n",
    "            !rm -rf /tmp/*.jpg\n",
    "            !rm -rf /tmp/*.png\n",
    "            !rm -rf /tmp/*.svg\n",
    "            gc.collect()\n",
    "            return self.score\n",
    "   \n",
    "    def normalize_data(self, data):\n",
    "        #return data\n",
    "        return (data - data.min()) / (data.max() - data.min())\n",
    "   \n",
    "    def download_data(self, ticker, start_date, end_date):\n",
    "        self.data = pdr.get_data_yahoo(ticker, start_date, end_date)\n",
    "        self.data = self.data.dropna()\n",
    "        print(self.data.head())\n",
    "        #normalize between 0 and 1\n",
    "        self.data = self.normalize_data(self.data['Adj Close'])\n",
    "        return self.data\n",
    "   \n",
    "    def rolling_average(self, data, window):\n",
    "        roll = data.rolling(window).mean()\n",
    "        #normalize between 0 and 1\n",
    "        roll = self.normalize_data(roll)\n",
    "        # correct for window size shift\n",
    "        #roll = roll.shift(-window)\n",
    "        return  roll\n",
    "    \n",
    "    def rolling_std(self, data, window):\n",
    "        roll = data.rolling(window).std()\n",
    "        #normalize between 0 and 1\n",
    "        roll = self.normalize_data(roll)\n",
    "        # correct for window size shift\n",
    "        #roll = roll.shift(-window)\n",
    "        return roll\n",
    "    \n",
    "    def gradient_at_each_point(self, data, window=False):\n",
    "        if window: \n",
    "            self.gradient = np.gradient(data.rolling(window))\n",
    "        else:\n",
    "            self.gradient = np.gradient(self.data)\n",
    "        self.gradient = pd.Series(self.gradient)\n",
    "    \n",
    "    def points_above_rollingaverage_and_bellow_rollingstd(self, data, window, rolling=False):\n",
    "        if rolling:\n",
    "            rolling_mean = self.rolling_average(data, window)\n",
    "            self.gradient_at_each_point(rolling_mean, window)\n",
    "            rolling_std = self.rolling_std(data, window)\n",
    "            self.points = data[(data > rolling_mean) & (data < rolling_std) & (self.gradient <= self.zeros.zeros)]\n",
    "        else:\n",
    "            rolling_std = self.rolling_std(data, window)\n",
    "            self.gradient_at_each_point(data, False)\n",
    "            self.points = data[(data < rolling_std) & (self.gradient <= self.zeros.zeros)]\n",
    "        if not self.TPTP:\n",
    "            self.TPTP, (TP, FP, FN, NUMBER_ANOMALIES, TOTAL_ANOMALIES, ANOMALLY_ASSIGNMENTS) = FP_TP(labels=self.label_list, preds=self.points)\n",
    "            self.score = FP/FN#/NUMBER_ANOMALIES\n",
    "            self.TOTAL_ANOMALIES = TOTAL_ANOMALIES\n",
    "            self.ANOMALLY_ASSIGNMENTS = ANOMALLY_ASSIGNMENTS\n",
    "            print(self.score)\n",
    "            return self.score\n",
    "\n",
    "    def plot(self, window=10, alpha=0.5, plots = ['data', 'mean', 'std', 'exponential', 'troughs', 'bet_std_mean', 'gradient']):\n",
    "        plt.figure(figsize=(int(len(self.data)/76*1.5), 7))\n",
    "\n",
    "        if 'mean' in plots:\n",
    "            rolling_mean = self.rolling_average(self.data, window)\n",
    "            plt.plot(rolling_mean, label='Rolling Mean')\n",
    "        if 'std' in plots:\n",
    "            rolling_std = self.rolling_std(self.data, window)\n",
    "            plt.plot(rolling_std, label='Rolling Std', color='green')\n",
    "        if 'exponential' in plots:\n",
    "            exponential_smoothing = self.exponential_smoothing(self.data, alpha)\n",
    "            plt.plot(exponential_smoothing, label='Exponential Smoothing')\n",
    "        if 'troughs' in plots:\n",
    "            troughs = self.lines_at_troughs(self.data, window)\n",
    "            plt.plot(troughs, label='Lines at Troughs')\n",
    "        if 'bet_std_mean' in plots:\n",
    "            self.points_above_rollingaverage_and_bellow_rollingstd(self.data, window, rolling=False)\n",
    "            # point y values should equal to data y values\n",
    "            plt.plot(self.points, 'ro', label='Flagged points', color='purple', markersize=10, alpha=0.5, marker='o')\n",
    "            # plot these points with y-values = data y values\n",
    "        #if 'prediction_bins' in plots:\n",
    "            ## uses the data we calculated in points_above_rollingaverage_and_bellow_rollingstd\n",
    "            #points_bins = self.TOTAL_ANOMALIES\n",
    "\n",
    "        if 'data' in plots:\n",
    "            plt.plot(np.arange(len(self.data)), self.data, color='black', label='data', linewidth=0.6)\n",
    "            plt.scatter(np.arange(len(self.data)), self.data, color='black', label='data', s=4)\n",
    "            plt.scatter(self.label_points['x'], self.label_points['y'], color='red', marker='x', s=10)\n",
    "        plt.legend(loc='upper left')\n",
    "        #return all plots\n",
    "        self.all_plots = plt.gcf()\n",
    "        #del plot\n",
    "        plt.close('all')\n",
    "        return self.all_plots\n",
    "\n",
    "    def show_plot(self, save=False, score_str=\"\"):\n",
    "        if save:\n",
    "            self.all_plots.savefig(f\"/home/smoothjazzuser/Desktop/output/folder-{self.name}_{methodd}_{score_str}.svg\")\n",
    "        plt.show()\n",
    "        return self.all_plots\n",
    "\n",
    "    def Train (self, c=channels, loss_sections=1, method='blur', method_ = 'recon', dataset_type='bugs', image_size=dims, downscale=True, epochs= 100):\n",
    "        os.chdir(\"/home/smoothjazzuser/videogame-anomoly/MNAD/\")\n",
    "\n",
    "        args = {'gpus': None, 'batch_size': 30, 'test_batch_size': 1, 'epochs': epochs, 'loss_compact': 0.15, 'loss_separate': 0.15, 'h': image_size, 'w': image_size, 'c': 3, 'lr': 2e-4, 'method': 'recon', 't_length': 1, 'fdim': 512, 'mdim': 512, 'msize': 10, 'num_workers': 15, 'num_workers_test': 1, 'dataset_type': 'bugs', 'dataset_path': './dataset', 'exp_dir': 'log', 'patience': 3}\n",
    "\n",
    "        gpus = \"0\"\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpus\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "        train_folder = args['dataset_path']+\"/\"+args['dataset_type']+\"/training/frames\"\n",
    "        test_folder = args['dataset_path']+\"/\"+args['dataset_type']+\"/testing/frames\"\n",
    "\n",
    "        # Loading dataset\n",
    "        train_dataset = DataLoader(train_folder, transforms.Compose([transforms.ToTensor(),]), resize_height=args['h'], resize_width=args['w'], time_step=args['t_length']-1)\n",
    "        test_dataset = DataLoader(test_folder, transforms.Compose([transforms.ToTensor(),]), resize_height=args['h'], resize_width=args['w'], time_step=args['t_length']-1)\n",
    "\n",
    "        train_size = len(train_dataset)\n",
    "        test_size = len(test_dataset)\n",
    "\n",
    "        train_batch = data.DataLoader(train_dataset, batch_size = args['batch_size'], shuffle=True, num_workers=args['num_workers'], drop_last=True)\n",
    "        test_batch = data.DataLoader(test_dataset, batch_size = args['test_batch_size'], shuffle=False, num_workers=args['num_workers_test'], drop_last=False)\n",
    "        \n",
    "        model = convAE(args['c'], memory_size = args['msize'], feature_dim = args['fdim'], key_dim = args['mdim'])\n",
    "        params_encoder =  list(model.encoder.parameters()) \n",
    "        params_decoder = list(model.decoder.parameters())\n",
    "        params = params_encoder + params_decoder\n",
    "        optimizer = torch.optim.Adam(params, lr = args['lr'])\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max =args['epochs'])\n",
    "        model.cuda()\n",
    "\n",
    "        log_dir = os.path.join('./exp', args['dataset_type'], args['method'], args['exp_dir'])\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "        if downscale:\n",
    "            def loss_func_mse(x, y):\n",
    "                x, y = F.interpolate(x, size=(args['h'], args['w'])), F.interpolate(y, size=(args['h'], args['w'])) #mode = nearest-exact\n",
    "                #fourier transform\n",
    "                x = x.squeeze()\n",
    "                y = y.squeeze()\n",
    "\n",
    "                loss = F.mse_loss(x,y)\n",
    "                return loss\n",
    "        else:\n",
    "            loss_func_mse = nn.MSELoss(reduction='none')\n",
    "\n",
    "        # Training\n",
    "\n",
    "        m_items = F.normalize(torch.rand((args['msize'], args['mdim']), dtype=torch.float), dim=1).cuda() # Initialize the memory items\n",
    "\n",
    "        e = 0\n",
    "        for epoch in tqdm(range(args['epochs'])):\n",
    "            e+=1\n",
    "            labels_list = []\n",
    "            model.train()\n",
    "            \n",
    "            start = time.time()\n",
    "            kkk = 0\n",
    "            #tqdm enumerate\n",
    "            for j,(imgs) in tqdm(enumerate(train_batch), total=len(train_batch)):\n",
    "                imgs = Variable(imgs).cuda()\n",
    "                outputs, _, _, m_items, softmax_score_query, softmax_score_memory, separateness_loss, compactness_loss = model.forward(imgs, m_items, True)\n",
    "                optimizer.zero_grad()\n",
    "                loss_pixel = torch.mean(loss_func_mse(outputs, imgs))\n",
    "                loss = loss_pixel + args['loss_compact'] * compactness_loss + args['loss_separate'] * separateness_loss\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            #early stopping ---> based on self.score\n",
    "            if not self.model: \n",
    "                self.model = model\n",
    "                self.m_items = m_items\n",
    "                self.best_score = 100000000000\n",
    "                self.patience = 0\n",
    "            else:\n",
    "                self.swap_test_folders(\"1001\")\n",
    "                self.Evaluate(metric_only=True) # update the predictions\n",
    "                self.analysis(metric_only=True, image_size=dims) #update self.score\n",
    "                os.makedirs('/tmp/empty', exist_ok=True)\n",
    "                !rsync -a --delete /tmp/empty/   /home/smoothjazzuser/Desktop/ram/temp/\n",
    "                pid = os.getpid()\n",
    "                os.system(\"kill -9 $(ps -o pid= --ppid %d)\" % pid)\n",
    "                gc.collect()\n",
    "                if self.score < self.best_score:\n",
    "                    self.patience = 0\n",
    "                    self.best_score = self.score\n",
    "                    self.model = model\n",
    "                    self.m_items = m_items\n",
    "                    self.best_epoch = epoch\n",
    "                else:\n",
    "                    self.patience += 1\n",
    "                    if self.patience > args['patience']:\n",
    "                        print(\"Early stopping\")\n",
    "                        model = self.model\n",
    "                        m_items = self.m_items\n",
    "                        break\n",
    "\n",
    "\n",
    "            \n",
    "            print('----------------------------------------')\n",
    "            print('Epoch:', epoch+1)\n",
    "            print('Loss: Reconstruction {:.6f}/ Compactness {:.6f}/ Separateness {:.6f}'.format(loss_pixel.item(), compactness_loss.item(), separateness_loss.item()))\n",
    "            print('Memory_items:')\n",
    "            print(m_items)\n",
    "            print('----------------------------------------')\n",
    "            \n",
    "        print('Training is finished')\n",
    "        if not os.path.exists(log_dir+temp_dir):\n",
    "            os.makedirs(log_dir+temp_dir, exist_ok=True)\n",
    "\n",
    "        torch.save(model, os.path.join(log_dir+temp_dir, 'model.pth'))\n",
    "        torch.save(m_items, os.path.join(log_dir+temp_dir, 'keys.pt')) \n",
    "\n",
    "        return self.best_score, self.best_epoch\n",
    "\n",
    "run = anomaly(method='recon', metric='anomaly_score', std_correction='false', dataset=\"bugs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut training dataset already exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(temp_dir.replace('temp', 'frames')):\n",
    "    run.swap_test_folders('all')\n",
    "    run.cut_dataset(keep_every=1)\n",
    "    print('cut dataset')\n",
    "\n",
    "else:\n",
    "    print('Cut training dataset already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1001']\n",
      "(1, 49272)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d5f9e2b001489a9b094ba1aee043ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249812ce125b4fdfbc29e0fde2293004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.swap_test_folders('1001')\n",
    "_, best_epoch = run.Train(epochs=100)\n",
    "\n",
    "\n",
    "if not os.path.exists('/home/smoothjazzuser/Desktop/output'):\n",
    "    os.mkdir('/home/smoothjazzuser/Desktop/output/')\n",
    "\n",
    "run.swap_test_folders('1000')\n",
    "for loss_type in ['median_blur', 'MSE', 'gaussian_blur']:\n",
    "    for win in [10, 30, 60, 90]:\n",
    "        FP_WINDOW  = win\n",
    "        for change in [5, 10, 20]:\n",
    "            CHANGE_WINDOW = change\n",
    "            for kernel in [5, 7, 9]:\n",
    "                KERNEL_SIZE = kernel\n",
    "\n",
    "                #load df\n",
    "                df = pd.read_csv('results.csv')\n",
    "\n",
    "                methodd = loss_type\n",
    "                run.Evaluate(image_size=dims)\n",
    "                run.analysis(video = video, metric_only=True)\n",
    "\n",
    "                # add data to df\n",
    "                df.add(run.score, FP_WINDOW, CHANGE_WINDOW, run.FP, run.TP, run.ANOMALIES, \"MNAD\", best_epoch, loss_type, \"MSE\", KERNEL_SIZE)\n",
    "\n",
    "                # remove any duplicates \n",
    "                df = df.drop_duplicates(subset=['SCORE','FP_WINDOW', 'CHANGE_WINDOW', 'FP', 'TP', 'ANOMALIES', 'ARCHITECHURE', 'EPOCHS', 'INFERENCE_LOSS_TYPE', 'TRAINING_LOSS_TYPE', \"KERNEL_SIZE\"], keep='first')\n",
    "\n",
    "                # del results.csv\n",
    "                if os.path.exists('results.csv'):\n",
    "                    os.remove('results.csv')\n",
    "                    try:\n",
    "                        df.to_csv('results.csv', index=False)\n",
    "                    except:\n",
    "                        df.to_csv(f'results{len(glob(\"./*.csv\") + glob(\"./*/*.csv\"))}.csv', index=False)\n",
    "                \n",
    "                \n",
    "                os.makedirs('/tmp/empty', exist_ok=True)\n",
    "                !rsync -a --delete /tmp/empty/   /home/smoothjazzuser/Desktop/ram/temp/\n",
    "                del run\n",
    "\n",
    "                #cleanup memory leak from multiprocessing\n",
    "                # get current procress id\n",
    "                pid = os.getpid()\n",
    "                #kill all child processes of pid\n",
    "                os.system(\"kill -9 $(ps -o pid= --ppid %d)\" % pid)\n",
    "                gc.collect()\n",
    "                run = anomaly(method='recon', metric='anomaly_score', std_correction='false', dataset=\"bugs\")\n",
    "\n",
    "                try:\n",
    "                    shutil.rmtree('/home/smoothjazzuser/Desktop/ram/', ignore_errors=True)\n",
    "                    os.mkdir(temp_dir)\n",
    "                    os.mkdir(temp_dir +'cleaned/')\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfbjfjd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folders_to_test = ['12', '23', '34', '54', '59', '60', '61','109', '117', '130']\n",
    "folders_to_test = [\n",
    "    '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70'\n",
    "    ]\n",
    "#folders_to_test = ['117']\n",
    "if not os.path.exists('/home/smoothjazzuser/Desktop/output'):\n",
    "    os.mkdir('/home/smoothjazzuser/Desktop/output/')\n",
    "\n",
    "for folder in folders_to_test:\n",
    "    run.swap_test_folders(folder)\n",
    "    run.Evaluate(image_size=dims)\n",
    "    run.analysis(video = video)\n",
    "    \n",
    "    os.makedirs('/tmp/empty', exist_ok=True)\n",
    "    !rsync -a --delete /tmp/empty/   /home/smoothjazzuser/Desktop/ram/temp/\n",
    "    del run\n",
    "\n",
    "    #cleanup memory leak from multiprocessing\n",
    "    # get current procress id\n",
    "    pid = os.getpid()\n",
    "    #kill all child processes of pid\n",
    "    os.system(\"kill -9 $(ps -o pid= --ppid %d)\" % pid)\n",
    "    gc.collect()\n",
    "    run = anomaly(method='recon', metric='anomaly_score', std_correction='false', dataset=\"bugs\")\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree('/home/smoothjazzuser/Desktop/ram/', ignore_errors=True)\n",
    "        os.mkdir(temp_dir)\n",
    "        os.mkdir(temp_dir +'cleaned/')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# delete all files containing \"-jvsc-\"\n",
    "!find ./ -name \"*-jvsc-*\" -type f -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current procress id\n",
    "pid = os.getpid()\n",
    "\n",
    "#kill all child processes of pid\n",
    "os.system(\"kill -9 $(ps -o pid= --ppid %d)\" % pid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kill all python\n",
    "!kill -9 $(ps -o pid= -C python)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7af08c789f4b74e9a00267d83d72d31580f58869f279fc3c57930ca1d6db349"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
